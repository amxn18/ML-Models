# ğŸ§  K-Nearest Neighbors (KNN) Classifier - From Scratch

This project demonstrates the implementation of the K-Nearest Neighbors (KNN) classification algorithm **from scratch in Python** and compares it with **scikit-learn's built-in KNN** model using the Iris dataset. It also includes visual performance comparisons through confusion matrices.

---

## ğŸ“Œ Algorithm: KNN

K-Nearest Neighbors is a **non-parametric**, **instance-based** learning algorithm. It classifies new data points based on **similarity (distance)** to the majority class among its `k` nearest neighbors in the training data.

---

## ğŸ§® Mathematics Used

- **Distance Metrics**:
  - ğŸ“ **Euclidean Distance**  
    \[
    d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + \ldots}
    \]
  - ğŸ™ï¸ **Manhattan Distance**  
    \[
    d = |x_1 - x_2| + |y_1 - y_2| + \ldots
    \]

- **No gradient descent**, **no cost function**, and **no model fitting** like in other ML models. KNN is a lazy learner â€” it simply stores the training data and calculates distances at prediction time.

---

## ğŸ—‚ Dataset Used

- **Iris Dataset** (from sklearn.datasets)
- 3 Classes: Setosa, Versicolor, Virginica
- 4 Features: Sepal Length, Sepal Width, Petal Length, Petal Width

---

## ğŸ”§ Workflow

1. **Load Dataset**
2. **Split** into Train & Test sets (80-20)
3. **Implement KNN from Scratch**
4. **Compare** with sklearn's KNN
5. **Evaluate Accuracy**
6. **Plot Confusion Matrices**

---

## ğŸš€ Usage

```python
# Custom KNN Classifier
model1 = KNNClassifier(distanceMetric="euclidean")
model1.fit(X_train, y_train)
y_pred_custom = model1.predict(X_test, k=3)

# Sklearn KNN Classifier
model2 = KNeighborsClassifier(n_neighbors=3)
model2.fit(X_train, y_train)
y_pred_sklearn = model2.predict(X_test)
```

---

## ğŸ“Š Evaluation

- âœ… **Accuracy (Custom KNN)**: ~95â€“100% (may vary)
- âœ… **Accuracy (Sklearn KNN)**: ~95â€“100%
- âœ… **Visual**: Confusion matrices plotted for both models using seaborn

---

## ğŸ“ˆ Confusion Matrix Sample Output

Left = Custom KNN | Right = Sklearn KNN

```text
       Predicted
        0  1  2
Actual
  0    âœ” âœ” âœ”
  1    âœ” âœ” âœ”
  2    âœ” âœ” âœ”
```

---

## ğŸ“ Files Included

- `knn_from_scratch.py` â€“ Main code
- `README.md` â€“ This file

---

## âœ… Learnings

- Difference between **lazy** (KNN) and **eager** (Logistic Regression, SVM) learning
- Role of **distance metrics**
- How KNN **does not train** the way parametric models do
- Visualization of classification results

---

## ğŸ§  Future Ideas

- Add support for **weighted KNN**
- Add **cosine similarity** as a distance metric
- Apply KNN to a **custom real-world dataset**

---

# ğŸ“Œ Note:
KNN works well for small datasets but may be slow for large datasets due to high prediction-time complexity:  
- **Time Complexity**: O(n Ã— d) per test sample (where n = training size, d = features)
- **Space Complexity**: O(n Ã— d)

---
